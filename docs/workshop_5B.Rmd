---
title: "Workshop 5B: Useful Life Distributions (Weibull, Gamma, & Lognormal)"
subtitle: "R Companion Code"
author: "Timothy Fraser"
date: "`r Sys.Date()`"
output:
 rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
    fig_caption: true
---

```{r setup, include = FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE)
```

In this workshop, we're going to continue learning some `R` functions for working with common life distributions, specifically the **Weibull, Gamma, and Log-Normal** distributions.

# 0. Getting Started

## 0.1 Load Packages

Let's start by loading the `tidyverse` package. We'll also load `mosaicCalc`, for taking derivatives and integrals (eg. `D()` and `antiD()`).

```{r}
# Load packages
library(tidyverse)
library(mosaicCalc)
```


## 0.2 Load Data

In this workshop, we're going to use a dataset of crop lifetimes! (Agriculture needs statistics too!) For each crop (`id`), we measured its lifespan in `days` from the day its seed is planted to the time it is ripe (usually about 90 days).

```{r, eval = FALSE, include = FALSE}
tibble(id = 1:50, days = rweibull(n = 50, shape = 1.5, scale = 80) %>% round(0)) %>%
  write_csv("/cloud/project/workshops/crops.csv")
```

```{r}
# Import crop lifespan data!
crops <- read_csv("/cloud/project/workshops/crops.csv")
```

<br>
<br>



# 1. Maximum Likelihood Estimation (MLE)

In our last few workshops, we learned that the exponential distribution does a pretty good job of approximating many life distributions, and that we can evaluate the fit of a distribution using a chi-squared test. In this workshop, we'll learn several alternative, related distributions that might fit your data even *better* depending on the scenario. But modeling any data using a distribution requires *that we know the parameters that best match the observed data*. This can be really hard, as we found out with $\hat{\lambda}$ in the exponential distribution.

Below, we'll learn a computational approach called Maximum Likelihood Estimation (MLE), that will help us find the true (most likely) values for any parameter in any dataset. It's pretty robust if the number of failures is 'large' enough, where 'large' counts as >10 (which is really small!). 

<br>
<br>

## Likelihood Function

MLE involves 3 ingredients:

1. `sample`: a vector of raw empirical data

2. `probability density function (PDF)`: tells us probability (relative frequency) of each value in a sample occurring.

3. `likelihood function`: tells us probability of getting this EXACT sample, given the PDF values for each observed data point.

We can get the **"probability"** of any sample by multiplying the density function `f(t)` at each data point $t_{i}$, for `r` data points. Multiplication indicates *joint* probability, so this product really means how likely is is to get **this specific sample**. We call it the **likelihood** of that sample.

$ LIK = \prod_{i = 1}^{r}{ f(t_i) } = f(t_1) \times f(t_2) \times ... f(t_n) $

```{r}
# Let's write a basic pdf function
d = function(t, lambda){lambda * exp(-t*lambda) }

# Let's imagine we already knew lambda...
mylambda = 0.01

crops %>%
  # Calculate probability density function for observed data
  mutate(prob = d(days, lambda = mylambda)) %>%
  summarize(
    # Let's calculate the likelihood
    likelihood = prob %>% prod(),
    # Since that's a really tiny number,
    # let's take its log to get log-Likelihood
    loglik = prob %>% prod() %>% log())
```
Technically, our `likelihood` above shows the joint probability of each of these observed values occurring together supposing that: 

1. *they have an exponential probability density function (PDF)*, and... 

2. that the parameter `lambda` in that PDF equals `0.01`. 

So we can get *different* (higher/lower) likelihoods of these values occuring together if we supply (1) different parameter values and therefore (2) a different hypothesized PDF. The most accurate parameters will be the ones that *maximize the likelihood*.

In practice though, the likelihood is teeny-tiny, and multiplying tiny numbers is hard! Fortunately...

1. the log of a tiny value makes it bigger and easier to read and compute.

2. the `log` of a product of values actually equals the *sum* of the `log` of those values (see equation below!).

$$ log ( \ L(t) \ ) = \log\prod_{i=1}^{n}{ f(t_i) = \sum_{i=1}^{n}\log( \ f(t_i) \ ) } $$ 

```{r}
ll = function(data, lambda){
  # Calculate Log-Likelihood,
  # by summing the log
  d(t = data, lambda) %>% log() %>% sum()
}
```

<br>
<br>

## Maximizing with `optim()`

We can use the `optim()` function from base `R` to:

- run our function (`fn` = ...) many times, varying the value of `lambda` (or any parameter).

- supply any other parameters like `data = crops$days` to each run.

- identify the log-likelihood that is *greatest* (if `control = list(fnscale = -1)`); by default, `optim()` actually *minimizes* otherwise.

- return the corresponding value for `lambda` (or any other parameter). 

- `optim()` will output a `$par` (our parameter estimate(s)) and `$value` (the maximum log-likelihood).

```{r}
# Maximize the log-likelihood!
optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1))
```

<br>
<br>

## Under the Hood

Wondering what's happening inside `optimize()`? Good news: it's not too tough. We can actually code and visualize it ourselves!

First, let's *get* all the log-likelihoods in that interval.

```{r}
# Let's 
manyll <- data.frame(parameter = seq(from = 0.00001, to = 1, by = 0.001)) %>%
  # For each parameter, get the loglikelihood
  group_by(parameter) %>%
  summarize(loglik = ll(data = crops$days, lambda = parameter))

# Check a few
manyll %>% head(3)
```

Let's maximize the log-likelihood manually...

```{r}
# Now find the parameter that has the greatest log-likelihood
output <- manyll %>%
  filter(loglik == max(loglik))

#check it
output
```
Now, let's visualize it!

```{r}
ggplot() +
  geom_line(data = manyll, mapping = aes(x = parameter, y = loglik), color = "steelblue") +
  geom_vline(xintercept = output$parameter, linetype = "dashed")  +
  theme_classic(base_size = 14) +
  labs(x = "parameter (lambda)", y = "loglik (Log-Likelihood)",
       subtitle = "Maximizing the Log-Likelihood (Visually)") +
  # We can actually adust the x-axis to work better with log-scales here
  scale_x_log10() +
  # We can also annnotate our visuals like so.
  annotate("text", x = 0.1, y = -2000, label = output$parameter)
```

## Multi-parameter optimization

Multi-parameter optimization works pretty similarly. We can use `optim()` for this too! 

- The key here is that `optim()` only varies whatever values we supply to `optim(par = ...)`. 

- So if we have multiple parameters, like the `mean` and `sd` for the normal distribution, we need to put both parameters into one vector in our input `par`, like so:

```{r}
# Let's write a new function
ll = function(data, par){
  # Our parameters input is now going to be vector of 2 values
  # par[1] gives the first value, the mean
  # par[2] gives the second value, the standard deviation
  dnorm(data, mean = par[1], sd = par[2]) %>% 
    log() %>% sum()
}
```

Now, let's maximize!

```{r}
# Let's optimize it!
# put in some reasonable starting values for 'par' and it optim() will do the rest. 
optim(par = c(90, 15), data = crops$days, fn = ll, control = list(fnscale = -1))
```

That was ridiculously easy! Let's compare to their known values:

```{r}
crops %>%
  summarize(mean = mean(days), sd = sd(days))
```
Almost spot on! Pretty good for an estimation technique!

<br>
<br>

# LC 1 {.tabset .tabs-pills}

## Question

We've learned several probability density functions for different distributions, including `dexp()`, `dgamma()`, `dpois()`, and `dweibull()`. Use `optim()` to maximize the likelihood of each of these PDFs' likelihood, using our `crops` data.

## Answer

For the exponential distribution...

```{r}
ll = function(data, par){
  dexp(data, rate = par) %>% log() %>% sum()
}
optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1))
```
For the gamma distribution...

```{r}
ll = function(data, par){
  dgamma(data, shape = par[1], scale = par[2]) %>% log() %>% sum()
}
optim(par = c(1, 1), data = crops$days, fn = ll, control = list(fnscale = -1))
```

For the Poisson distribution...

```{r}
ll = function(data, lambda){
  dpois(data, lambda) %>% log() %>% sum()
}
# Optimize!
optim(par = 90, data = crops$days, fn = ll, control = list(fnscale = -1))
```


For the Weibull distribution...

```{r}
ll = function(data, par){
  dweibull(data, shape = par[1], scale = par[2]) %>% log() %>% sum()
}
# Optimize!
optim(par = c(1,1), data = crops$days, fn = ll, control = list(fnscale = -1))
```

##

<br>
<br>

```{r}
remove(ll, d, output, mylambda, manyll)
```

<br>
<br>

# 2. Gamma Distribution

Alternatively, the Gamma distribution is well suited to modeling products exposed to a series of shocks over time at a given rate over time.

Excitingly, we can reapply most of the same rules we learned for exponential distributions; we just have to update our functions.

## $f(t)$ or `d(t)`(**Probability Density Function, aka PDF**)

In the gamma distribution, events are exposed to $k$ shocks that occur at rate $\lambda$ over time $t_1, t_2, ... t_n$. We can model the time to failure for such a function like so!

$$f(t) = \frac{\lambda}{(k - 1)!}(\lambda t)^{k-1}e^{-\lambda t}$$
We can also write $(k - 1)!$ above as $\Gamma(k)$. This is called a `"Gamma function of k"`, which is where the distribution's name comes from. $k$ effectively controls the `shape` of the function.

```{r}
# Let's write our new PDF function, d(t); written as f(t) above
d = function(t, k, lambda){
  lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda)
}

# Try it out!
d(t = 1, k = 1, lambda = 1)
# Compare with our dgamma() function!
dgamma(x = 1, shape = 1, rate = 1)
# They're the same!
```


<br>
<br>

## $F(t)$ **Failure Function** and $R(t)$ **Reliability Function**

The failure and reliability function are always closely related, no matter the distribution, where $R(t) = 1 - F(t)$. 

We can write the failure function $F(t)$ (a.k.a. the CDF) as:

$$ F(t) = 1 - \sum_{n = 0}^{k - 1}{\frac{(\lambda t)^n }{n!}e^{-\lambda t}}$$
Therefore, we can also write the reliability function $R(t)$ as:

$$ R(t) = \sum_{n = 0}^{k - 1}{\frac{(\lambda t)^n }{n!}e^{-\lambda t}}$$
```{r}
# Write the failure function for Gamma distribution
f = function(t, k, lambda){
  # Make a vector of values from 0 to k-1
  n = seq(from = 0, to = k - 1)
  # Now compute the failure function
  1 - sum( (lambda*t)^n / factorial(n)  * exp(-lambda*t) )
}

# We can also integrate the PDF to get the CDF, alternatively, using mosaicCalc
fc = antiD(d(t, k, lambda) ~ t)

# Let's compare!
# Using pgamma()
pgamma(q = 10,shape = 3,rate = 1)
# Using our calculate-based fc()
fc(t = 10, k = 3, lambda = 1)
# Using our direct function f()
f(t = 10, k = 3, lambda = 1)
# All the same!
```
Correspondingly, the reliability function $R(t)$ would be...

```{r}
# Write the reliability function
r = function(t, k, lambda){
  # Make a vector of values from 0 to k-1
  n = seq(from = 0, to = k - 1)
  # Now compute the reliability function
  sum( (lambda*t)^n / factorial(n)  * exp(-lambda*t) )
}

# Get reliability function via calculus...
rc = function(t,k,lambda){
  # compute CDF
  fc = antiD(d(t, k, lambda) ~ t)
  # return 1 - CDF
  1 - fc(t,k,lambda)
}


# Compare outputs!
# with pgamma()
1 - pgamma(q = 10, shape = 3, rate = 1)
# with rc()
rc(t = 10, k = 3, lambda = 1)
# with r()
r(t = 10, k = 3, lambda = 1)
# They're the same!
```

<br>
<br>

$z(t)$ Failure Rate function

The failure rate remains $z(t) = \frac{f(t)}{R(t)}$. Further, in the gamma distribution, the rate remains constant, although the size parameter causes a result quite different from the exponential!

```{r}
# Let's write the failure rate function
z = function(t,k,lambda){
  # We'll break it up into parts to help readability
  
  # Compute the PDF d(t) (or f(t))
  d_of_t = lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda)
  
  # Make a vector of values from 0 to k-1
  n = seq(from = 0, to = k - 1)
  
  # Now compute the reliability function R(t)
  r_of_t = sum( (lambda*t)^n / factorial(n)  * exp(-lambda*t) )
 
  # Divide the two to get the failure rate!
  d_of_t / r_of_t
}

# Or, using calculus....
zc = function(t, k, lambda){
  # Get CDF via integration
  fc = antiD(d(t, k, lambda) ~ t)
  # Get r(t) using 1 - f(t)
  # Take PDF / (1 - CDF)
  d(t, k, lambda) / (1 - fc(t, k, lambda) )
}

# Try it out!

# Using dgamma() and pgamma()
dgamma(1,shape = 1, rate = 0.1) / 
  (1 - pgamma(1,shape = 1, rate = 0.1))

# Using our calculus based function
zc(t = 1, k = 1, lambda = 0.1)

# Using our home cooked function
z(t = 1, k = 1, lambda = 0.1)
```

<br>
<br>

## MTTF (Mean Time to Fail) and Variance

We can also compute the mean time to fail and variance using the following formulas.

$$ MTTF = \int_{0}^{\infty}{ t f(t) dt} = \frac{k}{\lambda}$$
$$ Var(t) = \frac{k}{\lambda^2} $$
We can encode these as functions as follows.

```{r}
# Mean (aka mean time to fail)
mttf = function(k, lambda){   k / lambda  }
# Check it!
mttf(k = 1, lambda = 0.1)
```

```{r}
# Variance
variance = function(k, lambda){ k / lambda^2 }
# Try it!
variance(k = 1, lambda = 0.1)
```

<br>
<br>

# LC 2 {.tabset .tabs-pills}

## Question

A car bumper has demonstrated a gamma distribution with a failure rate of $\lambda = 0.005$ per day given $k = 3$ potential shocks. Determine the reliability of this bumper over a 30, 300, and 600 day period.

## Answer

```{r}
# Compute a reliability function using pgamma()
r = function(t,k,lambda){ 1 - pgamma(t,shape = k, rate = lambda) }

# For 24 hours
r(t = c(30, 300, 600), k = 3, lambda = 0.005)
```

##

<br>
<br>


# 3. Weibull Distribution

Another popular distribution is the Weibull distribution. The exponential is a special case of Weibull distribution where the failure rate $\lambda$ is held constant such that $m = 1$. But in a usual Weibull distribution, the failure rate can change over time! This makes it very flexible, able to take on the shape of an exponential, gamma, or normal distribution depending on the parameters.

We can show this easily using the Weibull's cumulative hazard function $H(t) = (\lambda t)^m$. If $m = 1$, as in the exponential distribution, then $H(t) = \lambda t$, as in the exponential distribution. But if $m \neq 1$, then the accumulative hazard rate increases to the $m$ power with ever passing hour. 

We can use this to derive the failure function $F(t)$, reliability function $R(t)$, and failure rate $z(t)$. (Similarly, we can use all the same tricks from before to get the $AFR(t_1, t_2)$, like using the accumulative hazard function $\frac{H(t_2) - H(t_1)}{t_2 - t_1}$.)

<br>
<br>

## $F(t)$ Failure Function and $R(t)$ Reliability Function

To derive $F(t)$, we can sub in the new formula for the accumulative hazard function into the formula for the failure function. The Weibull failure function is also commonly written replacing $\lambda$ with $c = \frac{1}{\lambda}$, the *characteristic life*.

$$ F(t) = 1 - e^{-H(t)} = 1 - e^{-(\lambda t)^m} = 1 - e^{-(t/c)^m}$$
We can code it like so:

```{r}
f = function(t, m, c){ 1 - exp(-1*(t/c)^m) }
# Compare!
# Using pweibull()
pweibull(1, shape = 1, scale = 1)
# Using our home-made function!
f(t = 1, m = 1, c = 1)
```
Similarly, we can write the reliability function as...

```{r}
r = function(t, m, c){ exp(-1*(t/c)^m) }

# Try it with pweibull()
1 - pweibull(1, shape = 1, scale = 1)
# Using our home-made function!
r(t = 1, m = 1, c = 1)
```

## $f(t)$ or `d(t)` (PDF)

While not nearly as straightforward to derive, we can literally take the derivative of the Failure function to get the PDF.

$$ f(t) = \frac{m}{t} \times (\frac{t}{c})^m \times e^{-(t/c)^m}$$
```{r}
d = function(t, m, c){
  (m / t) * (t / c)^m * exp(-1*(t/c)^m)
  # alternatively written using calculus:
  # D(f(t,m,c) ~ t)
}

# Compare results!
dweibull(1, shape = 1, scale = 1)
d(t = 1, m = 1, c = 1)
```
## $z(t)$ Failure Rate

We can similarly *derive* the failure rate $z(t)$ by taking the derivative of the accumulative hazard rate, which gives us:

$$z(t) = m \lambda (\lambda t)^{m-1} = (m/c) \times (t/c)^{m-1} $$/'

```{r}
z = function(t, m, c){
  (m/c) * (t/c)^(m-1)
}

# try it!
z(1, m = 1, c = 0.1)

# using dweibull() and pweibull()
dweibull(1, shape = 1, scale = 0.1) /
  (1 - pweibull(1, shape = 1, scale = 0.1)) 
```
<br>
<br>

## $m$ and $c$

Fortunately, if any 3 of the 4 parameters ($F(t), m, t, c$ are known about a Weibull distribution, the remaining parameter can be calculated. Since $F(t) = 1 - e^{-(t/c)^m}$, we can say:

- $t = c( -log( 1 - F ))^{1/m} $

- $m = \frac{log( -log( 1 - F) )}{log(t/c)}$

- $c = \frac{t}{( -log(1 - F) )^{1/m}}$

- Characteristic life $c$, in this case, really means the time at which 63.2% of units will consistently have failed.

- Shape parameter $m$ is used to describe several different types of Weibull distributions. $m = 1$ is an exponential; $m = 2$ is a Reyleigh distribution, where the failure rate increases linearly. When $m < 1$, it looks like the left-end of a bath-tub. When $m > 1$, it looks like the right-end of a bath-tub.

<br>
<br>


## Weibull Series Systems

In a series system of independent components, which are each Weibull distributed with the same shape parameter $m$, that system has a Weibull distribution, as follows:

$$ c_{series} = (\sum_{i=1}^{n}{ \frac{1}{c_i^m}})^{-\frac{1}{m}} $$

<br>
<br>
# LC 3 {.tabset .tabs-pills}

## Question

You've done this a bunch now - for this learning check, write your own function to find $t$, $m$, and $c$ in a Weibull distribution.

## Answer

```{r}
# Write a function to get t
get_t = function(f,m,c){
  log(-log(1 - f)  )/log(t/c)
}

```

```{r}
# Write a function to get m
get_m = function(t,f,c){
  log(-log(1 - f)  )/log(t/c)
}
```

```{r}
# Write a function to get c
get_c = function(t,f,m){
  t / ( (-log(1 - f))^(1/m) )
}
```

##


<br>
<br>

# LC 4 {.tabset .tabs-pills}

## Question

- Find the characteristic life necessary for 10% of failures to occur by 168 hours, if the shape parameter $m = 2$.

- Then, using that characteristic life, plot the probability of failure when $m = 1$, $m = 2$, and $m = 3$.

## Answer

```{r}
# Write our function to find c
get_c = function(t, f, m){
  t / ( (-log(1 - f))^(1/m) )
}

get_c(t = 168, f = 0.10, m = 2)
# Looks like we expect a characteristic life of >500 hours. 
```
```{r}
# Write the failure function
f = function(t, c, m){ 1 - exp(-1*(t/c)^m) }

m1 <- data.frame(hours = 1:100) %>%
  mutate(prob = f(t = hours, c = 517.5715, m = 1),
         m = "m = 1")

m2 <- data.frame(hours = 1:100) %>%
  mutate(prob = f(t = hours, c = 517.5715, m = 2),
         m = "m = 2")

m3 <- data.frame(hours = 1:100) %>%
  mutate(prob = f(t = hours, c = 517.5715, m = 3),
         m = "m = 3")

bind_rows(m1,m2,m3) %>%
  ggplot(mapping = aes(x = hours, y = prob, color = m)) +
  geom_line()
```


## 

<br>
<br>

# Lognormal Distribution

The log-normal distribution can be very useful, often in modeling semi-conductors, among other product.

To understand the log-normal distribution, let's outline the normal distribution PDF, and compare it to the log-normal distribution PDF.

The normal distribution contains the parameters $\mu$ (mean) and $\sigma$ (standard deviation).

$$f(t) = \frac{1}{\sigma \sqrt{2\pi} } \times e^{-(t - \mu)^2 / 2\sigma^2 } $$
The log-normal distribution is quite similar. Here, $t$ becomes $e^t$ and median $T_{50} = e^{\mu}$. $\sigma$ is really more of a shape parameter here than the standard deviation as we usually think of it.

$$ f(t) = \frac{1}{\sigma t \sqrt{2\pi}} \times e^{-(1/2\sigma^2) \times (log(t) - log(T_{50} ))^2}$$
This one's prety messy, so to avoid parenthesis errors, we're just going to jump straight to using the `dlnorm()` function, which contains 2 parameters, `meanlog` and `sdlog`.

```{r}
# Write the pdf of the log-normal, f(t) or d(t)!
d = function(t, mu, sigma){ dlnorm(t, meanlog = mu, sdlog = sigma)  }

# Write the failure function of the log-normal F(t)!
f = function(t, mu, sigma){ plnorm(t, meanlog = mu, sdlog = sigma) }

# Write the reliability function R(t)!
r = function(t, mu, sigma){ 1 - plnorm(t, meanlog = mu, sdlog = sigma) }

# Write the failure rate z(t)!
z = function(t, mu, sigma){  d(t, mu, sigma) / r(t, mu, sigma)  }

# Write the accumulative hazard function H(t)!
h = function(t, mu, sigma){ -log(r(t, mu, sigma))  }

# Write the average failure rate function AFR(t)
afr = function(t1, t2, mu, sigma){ (h(t2) - h(t1) ) / (t2 - t1)  }
```

Wow! That was surprisingly easy! In this way, as we pick up more and more distributions in this course, our tools for generating failure and reliability functions get easier and easier too!

<br>
<br>

# LC 5 {.tabset .tabs-pills}

## Question

A crop tends to grow to a median height of 2 feet tall. We know from past data that this crop has a lognormal distribution, with a shape parameter $\sigma$ of about 0.2 feet. Market standards prefer crops between 1 and 8 feet tall. Given these standards, what percentage of crops are not eligible for market?

## Answer

```{r}
# Get percent under 1.5 feet
below <- plnorm(1, meanlog = 2, sdlog = 0.2)

# Get percentage over 6 feet
above <- 1 - plnorm(8, meanlog = 2, sdlog = 0.2)

# Get total percentage outside of these bounds
below + above

# As a visual example...
rlnorm(1000, meanlog = 2, sdlog = 0.2) %>% hist()
```

##

<br>
<br>

## MTTF and Variance

Finally, we can quickly calculate these quantities of interest too.

- $ MTTF = T_{50} \times e^{\sigma^2 / 2} $

- $ Variance = T_{50} \times e^{\sigma^2} \times (e^{\sigma^2} - 1)$

<br>
<br>

# LC 6 {.tabset, .tabset-pills}

## Question

A log normal distribution has a median time to failure equal to 50,000 hours and $\shape$ parameter sigma equal to 0.8. What is the mean time to failure and true standard deviation in this distribution?

## Answer

```{r}
# Let's write a mean time to failure function
mttf = function(median, sigma){  median * exp(sigma^2 / 2)  }

mttf(median = 50000, sigma = 0.8)
# Looks like the MTTF is ~69,000 hours
```

```{r}
# Let's write the variance function
variance = function(median, sigma){
  median^2 * exp(sigma^2 / 2)  * (exp(sigma^2) - 1)
}

# Now calculate variance, and take square root to get sd
variance(median = 50000, sigma = 0.8) %>% sqrt()
# with a very wide standard deviation
```

## 

<br>
<br>

# Conclusion

Great work! You've learned to work with the Exponential, Gamma, Weibull, and Log-normal distribution. You're ready to start coding some cool systems reliability analyses! Next week, we'll learn some new techniques to help!

